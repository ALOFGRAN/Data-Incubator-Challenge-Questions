{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __OLD DASK/CHUNKING EFFORTS__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Merging time:__\n",
    "Seeing as I was able to reduce the price dataset by almost half, I'll try and merge the two datasets again.  I'll merge 'right' this time because I want to keep as much of the patent data as possible. \n",
    "\n",
    "<b>Note to self:</b> you may want to come back and merge by 'outer' so that you can retain as much price info as possible and extrapolate any missing patent data for drugs that have prices but no patent dates.\n",
    "\n",
    "Originally, I opened the fuzzy_prices file at the top cleaned it, and then tried to priocess it here at the bottom of the notebook. I've found, however, that chunking and processing the CSV as it is read in leads to far fewer errors with my machine's limited memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk, prepare, and merge the fuzzy_prices file with the all_data file\n",
    "for chunk in pd.read_csv('fuzzy_prices.csv', chunksize = 25e6, engine = 'python'):\n",
    "    chunk.set_index('ndc_description_agg')\n",
    "    chunk.drop(['Unnamed: 0', \n",
    "                       'ndc', \n",
    "                       'corresponding_generic_drug_nadac_per_unit',\n",
    "                       'corresponding_generic_drug_effective_date',\n",
    "                       'Unnamed: 0.1'], axis = 1, inplace = True)\n",
    "    chunk.head(3)\n",
    "\n",
    "    # Convert to datetime and see the distribution of dates in the 'effective_date' column\n",
    "    pd.to_datetime(chunk['effective_date'])\n",
    "    chunk['effective_date'].value_counts(dropna = False).sort_values(ascending = False)\n",
    "\n",
    "    # Attempting to lighten up the dataset further by dropping duplicates\n",
    "    chunk.drop_duplicates(keep='first')\n",
    "    chunk.info()\n",
    "    merged_all = chunk.join(all_data, how = 'outer')\n",
    "    merged_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the merged file\n",
    "merged_all = merged_all.to_csv('merged_all.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Yet another Dask attempt__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempting dask again!\n",
    "from dask import dataframe as dd \n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "# Initiate the client!\n",
    "client = Client(n_workers = 1, \n",
    "                threads_per_worker = 4, \n",
    "                processes = False, \n",
    "               memory_limit = '14GB', \n",
    "               scheduler_port = 0, \n",
    "               silence_logs = True, \n",
    "               diagnostics_port = 0)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the merger\n",
    "merged_all_ddf = client.submit(pd.merge(fuzzy_prices, all_data, on=['ndc_description_agg'], how = 'right').compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_all_ddf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2.0\n",
    "\n",
    "# Define matching function that will be used to provide a comparison of strings (drug names, strengths, and routes) for later merging of datasets\n",
    "def match_name(name, list_names, min_score=0):\n",
    "    # -1 score incase we don't get any matches\n",
    "    max_score = -1\n",
    "    # Returning empty name for no match as well\n",
    "    max_name = \"\"\n",
    "    # Iternating over all names in the other\n",
    "    for name2 in list_names:\n",
    "        #Finding fuzzy match score\n",
    "        score = fuzz.token_set_ratio(name, name2)\n",
    "        # Checking if we are above our threshold and have a better score\n",
    "        if (score > min_score) & (score > max_score):\n",
    "            max_name = name2\n",
    "            max_score = score\n",
    "    return (max_name, max_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Name matching__\n",
    "I've found that very little of the data in the drug pricing dataset and the patent dataset overlaps.  This is good and bad.  Good, because it gives me more data to play with.  Bad because it'll be more difficult to match up the data in each set.\n",
    "\n",
    "I've found that there's a python package called 'fuzzywuzzy' which produces a Levenshtein score (effectively a way to compare the similarity of two strings).  I plan to use the score as I compare the ndc_description (read: drug name) from one dataset to an aggregate of three columns in the other dataset (trade_name, strength, route) that should produce a similar drug name.\n",
    "\n",
    "Because I had a lot of problems with the processing of these fuzzy strings, I had to break them up into batches so that I'd have more control over the process (than a loop would give me)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2.1 - works (w/o Dask!)\n",
    "# Runs the function above\n",
    "# List for dicts for easy dataframe creation\n",
    "dict_list = []\n",
    "# iterating over our drugs to find a match\n",
    "for name in new_prices['ndc_description'][:1000]:\n",
    "    # Use our method to find best match, we can set a threshold here\n",
    "    match = match_name(name, new_all_data['ndc_description_agg'], 85)\n",
    "    \n",
    "    # New dict for storing data\n",
    "    dict_ = {}\n",
    "    dict_.update({'ndc_description' : name})\n",
    "    dict_.update({'ndc_description_agg' : match[0]})\n",
    "    dict_.update({'score' : match[1]})\n",
    "    dict_list.append(dict_)\n",
    "    \n",
    "merge_table1 = pd.DataFrame(dict_list)\n",
    "# Display results\n",
    "merge_table1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2.1 - works (w/o Dask!)\n",
    "# Runs the function above\n",
    "# List for dicts for easy dataframe creation\n",
    "dict_list = []\n",
    "# iterating over our drugs to find a match\n",
    "for name in new_prices['ndc_description'][1001:2000]:\n",
    "    # Use our method to find best match, we can set a threshold here\n",
    "    match = match_name(name, new_all_data['ndc_description_agg'], 85)\n",
    "    \n",
    "    # New dict for storing data\n",
    "    dict_ = {}\n",
    "    dict_.update({'ndc_description' : name})\n",
    "    dict_.update({'ndc_description_agg' : match[0]})\n",
    "    dict_.update({'score' : match[1]})\n",
    "    dict_list.append(dict_)\n",
    "    \n",
    "merge_table2 = pd.DataFrame(dict_list)\n",
    "# Display results\n",
    "# merge_table2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2.1 - works (w/o Dask!)\n",
    "# Runs the function above\n",
    "# List for dicts for easy dataframe creation\n",
    "dict_list = []\n",
    "# iterating over our drugs to find a match\n",
    "for name in new_prices['ndc_description'][2001:3000]:\n",
    "    # Use our method to find best match, we can set a threshold here\n",
    "    match = match_name(name, new_all_data['ndc_description_agg'], 85)\n",
    "    \n",
    "    # New dict for storing data\n",
    "    dict_ = {}\n",
    "    dict_.update({'ndc_description' : name})\n",
    "    dict_.update({'ndc_description_agg' : match[0]})\n",
    "    dict_.update({'score' : match[1]})\n",
    "    dict_list.append(dict_)\n",
    "    \n",
    "merge_table3 = pd.DataFrame(dict_list)\n",
    "# Display results\n",
    "# merge_table3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2.1 - works (w/o Dask!)\n",
    "# Runs the function above\n",
    "# List for dicts for easy dataframe creation\n",
    "dict_list = []\n",
    "# iterating over our drugs to find a match\n",
    "for name in new_prices['ndc_description'][3001:4000]:\n",
    "    # Use our method to find best match, we can set a threshold here\n",
    "    match = match_name(name, new_all_data['ndc_description_agg'], 85)\n",
    "    \n",
    "    # New dict for storing data\n",
    "    dict_ = {}\n",
    "    dict_.update({'ndc_description' : name})\n",
    "    dict_.update({'ndc_description_agg' : match[0]})\n",
    "    dict_.update({'score' : match[1]})\n",
    "    dict_list.append(dict_)\n",
    "    \n",
    "merge_table4 = pd.DataFrame(dict_list)\n",
    "# Display results\n",
    "# merge_table4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2.1 - works (w/o Dask!)\n",
    "# Runs the function above\n",
    "# List for dicts for easy dataframe creation\n",
    "dict_list = []\n",
    "# iterating over our drugs to find a match\n",
    "for name in new_prices['ndc_description'][4001:5000]:\n",
    "    # Use our method to find best match, we can set a threshold here\n",
    "    match = match_name(name, new_all_data['ndc_description_agg'], 85)\n",
    "    \n",
    "    # New dict for storing data\n",
    "    dict_ = {}\n",
    "    dict_.update({'ndc_description' : name})\n",
    "    dict_.update({'ndc_description_agg' : match[0]})\n",
    "    dict_.update({'score' : match[1]})\n",
    "    dict_list.append(dict_)\n",
    "    \n",
    "merge_table5 = pd.DataFrame(dict_list)\n",
    "# Display results\n",
    "# merge_table5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all fuzzy merged files (if you turn this on, turn the code in the next cell down off)\n",
    "# frames = [merge_table1, merge_table2, merge_table3, merge_table4, merge_table5]\n",
    "# all_merged = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bring in all tables instead of run the loops to generate fuzz scores again (turn this off if you want to run the cell immediately above)\n",
    "merge_table1 = pd.read_csv('merge_table1')\n",
    "merge_table2 = pd.read_csv('merge_table2')\n",
    "merge_table3 = pd.read_csv('merge_table3')\n",
    "merge_table4 = pd.read_csv('merge_table4')\n",
    "merge_table5 = pd.read_csv('merge_table5')\n",
    "merge_all = pd.concat([merge_table1, merge_table2, merge_table3, merge_table4, merge_table5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzy_prices = pd.merge(prices, merge_all, on = ['ndc_description'], how = 'inner')\n",
    "fuzzy_prices.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up a bit to free up some space\n",
    "del merge_table1\n",
    "del merge_table2\n",
    "del merge_table3\n",
    "del merge_table4\n",
    "del merge_table5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the size of fuzzy_prices by taking out any values that don't have a high match (fuzz) score\n",
    "fuzzy_prices = fuzzy_prices[fuzzy_prices['score'] >= 85]\n",
    "fuzzy_prices.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Crashes system due to low memory\n",
    "all_merged_data = pd.merge(fuzzy_prices, new_all_data, on = ['ndc_description_agg'], how = 'inner')\n",
    "all_merged_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've learned that it's very helpful to regularly export your data if you're frequently maxing out your machine's capabilities :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export all fuzz files (only need if fuzz is running particularly slow)\n",
    "merge_table1 = merge_table1.to_csv('merge_table1')  #processed (records :1000)\n",
    "merge_table2 = merge_table2.to_csv('merge_table2')  #processed (records 1001:2000)\n",
    "merge_table3 = merge_table3.to_csv('merge_table3')  #processed (records 2001:3000)\n",
    "merge_table4 = merge_table4.to_csv('merge_table4')  #processed (records 3001:4000)\n",
    "merge_table5 = merge_table5.to_csv('merge_table5')  #processed (records 3001:4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export all merged files (if you could process them all together)\n",
    "fuzzy_prices = fuzzy_prices.to_csv('fuzzy_prices')\n",
    "all_data = all_data.to_csv('all_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data from all files above as single file (if you could process them all together)\n",
    "all_merged_data = all_merged_data.to_csv('all_merged_data')  #prices, patents, products, exclusivity files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    <p>\n",
    "        <p>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# __Everything Beyond this point is an effort to quicken the above processes with Dask (parallel processing)__\n",
    " \n",
    "             \n",
    "             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "client = Client()\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_ddf = dd.from_pandas(prices, npartitions=1)\n",
    "all_data_ddf = dd.from_pandas(all_data, npartitions=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_filtered_ddf = dd.from_pandas(prices_filtered, chunksize = 25e6) #prices_filtered: 404.2MB\n",
    "all_data_ddf = dd.from_pandas(all_data, chunksize = 25e6) #all_data: 88.7MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1.0\n",
    "def fuzzy_score(str1, str2):\n",
    "    return fuzz.token_set_ratio(str1, str2)\n",
    "\n",
    "def helper(orig_string, slave_df): # add Client in here?\n",
    "    slave_df['score'] = slave_df['ndc_description_agg'].apply(lambda x: fuzzy_score(x,orig_string))\n",
    "    #return my_value corresponding to the highest score\n",
    "    return slave_df.loc[slave_df.ndc_description_agg.idxmax(),'ndc_description']\n",
    "\n",
    "dmaster = dd.from_pandas(all_data, npartitions=8) # add Client in here?\n",
    "dmaster['ndc_description'] = dmaster.ndc_description.apply(lambda x: helper(prices_filtered_ddf, prices_filtered_ddf, meta=(x, 'f8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1.1\n",
    "# dmaster.computer(schedule = 'processes')  #original line of code\n",
    "final = dmaster.scatter  #try this instead\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2.0 (dask starts below)\n",
    "\n",
    "# Define matching function\n",
    "def match_name(name, list_names, min_score=0):\n",
    "    # -1 score incase we don't get any matches\n",
    "    max_score = -1\n",
    "    # Returning empty name for no match as well\n",
    "    max_name = \"\"\n",
    "    # Iternating over all names in the other\n",
    "    for name2 in list_names:\n",
    "        #Finding fuzzy match score\n",
    "        score = fuzz.token_set_ratio(name, name2)\n",
    "        # Checking if we are above our threshold and have a better score\n",
    "        if (score > min_score) & (score > max_score):\n",
    "            max_name = name2\n",
    "            max_score = score\n",
    "    return (max_name, max_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2.1 - (trying w/ Dask!)\n",
    "# Runs the function above\n",
    "gc.collect()\n",
    "new_prices_filtered_ddf = dd.from_pandas(new_prices, chunksize = int(25e6) #prices_filtered: 404.2MB\n",
    "new_all_data_ddf = dd.from_pandas(new_all_data, chunksize = int(25e6) #all_data: 88.7MB\n",
    "\n",
    "# List for dicts for easy dataframe creation\n",
    "dict_list = []\n",
    "# iterating over our drugs to find a match\n",
    "for name in new_prices_filtered_ddf['ndc_description_agg'][:100]:\n",
    "    # Use our method to find best match, we can set a threshold here\n",
    "    match = match_name(name, new_prices_filtered_ddf['ndc_description'], 85)\n",
    "    \n",
    "    # New dict for storing data\n",
    "    dict_ = {}\n",
    "    dict_.update({'ndc_description_agg' : name})\n",
    "    dict_.update({'ndc_description' : match[0]})\n",
    "    dict_.update({'score' : match[1]})\n",
    "    dict_list.append(dict_)\n",
    "    \n",
    "merge_table = pd.DataFrame(dict_list)\n",
    "# Display results\n",
    "merge_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 3.1\n",
    "# Merge two dataframes (new_prices, new_all_data) and call .applymap() - applies to the entire with a lambda calling the fuzzy_score function defined above\n",
    "# Merge\n",
    "agg_names = new_prices + new_all_data\n",
    "\n",
    "# Call \n",
    "agg_names['score'] = agg_names['ndc_description_agg'].applymap(lambda x: x.fuzz(x, ndc_description),ndc_description_agg)\n",
    "                                                          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 4 - my approach\n",
    "test_data = []\n",
    "for each in prices_filtered['ndc_description'][100]:\n",
    "    a = 1\n",
    "    b = 1\n",
    "    while a < 101: #len(all_data['ndc_description_agg'])\n",
    "        testing = all_data['ndc_description_agg'][a]\n",
    "        rating = fuzz.ratio(testing, each) # Compare the two strings and save the result\n",
    "        # print(rating, end='\\r')\n",
    "        if rating >= 80:\n",
    "            test_data.append([each, all_data['ndc_description_agg'], rating])\n",
    "            #prices_filtered.append(each, inplace = True)\n",
    "        a += 1\n",
    "    b += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
